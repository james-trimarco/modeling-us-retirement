---
title: "Modeling U.S. Retirement with Logistic Regression"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})

author: "James Trimarco"
date: "5/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dev="png", out.width = '90%')
```

## Part 1: Setup
```{r set_contrasts, include=FALSE}
options(contrasts = c("contr.treatment", "contr.treatment"))
options('contrasts')
```

### Import libraries
```{r import_libs, warning=FALSE, message=FALSE}
library(tidyverse) # all-purpose tools
library(broom) # converting models to dataframes
library(here) # finding source files
library(survey) # for handling weighted survey data
library(srvyr) # allows tidyverse syntax on survey objects
library(tools) # odds and ends
library(caret) # for sensitivity and specificity calculations
library(wesanderson) # nice colors
library(glue) # nice concatenation
```

### Obtain fonts
```{r add_fonts}
library(showtext)
## Loading Google fonts (http://www.google.com/fonts)
font_add_google("spectral", "spectral")
font_add_google("roboto slab", "roboto")
```

### Theme for plots
```{r create_theme}
theme_set(theme_minimal() + theme(text=element_text(size=12, family="roboto"), 
      plot.title = element_text(size=16, vjust=2),
      plot.margin = unit(c(1,1,b = 1,1), "cm"),
      plot.caption = element_text(size = 8, family = "spectral", vjust = -3),
      axis.title.y= element_text(vjust = 2), 
      axis.title.x = element_text(margin = margin(t = 0, r = 0, 
                                                  b = 0, l = 0, unit = "cm"), vjust = -2)))
```

### Read in the data
I'm not including the data in this GitHub repo because it's quite large. To run the code below, simply download the data from [this link](http://gss.norc.org/documents/stata/GSS_stata.zip) and point `mypath` to your local copy. 
```{r importGSS, cache = TRUE}
mypath <- here::here("GSS_spss", "GSS_stata", "GSS7216_R4.DTA")
gss_all <- foreign::read.dta(mypath)
```

## Part 2: Preparing the data
### Define variables of interest  
The GSS comes with more than 5,000 variables, most of which have little or nothing to do with the age at which a person will retire. Most of the ones I'm selecting here — including age, race, sex, and marital status — have all proved to result in statistically significant coefficients in logistic regression. 

A few of the variables, like `race` and `degree`, did not produce statistically significant coefficients, but I'm selecting them anyway so that they'll be included in the complex survey design and are available for exploration. 

We'll also keep all the variables used to create the survey design, including weights and strata.

```{r select_vars}
cont_vars <- c("year", "id", "age")

cat_vars <- c("race", "sex", "wrkstat", "degree", "marital", 
              "born", "wrkgovt")

wt_vars <- c("vpsu",
             "vstrat",
             "oversamp",
             "formwt",              # weight to deal with experimental randomization
             "wtssall",             # weight variable
             "sampcode",            # sampling error code
             "sample")              # sampling frame and method

vars <- c(cont_vars, cat_vars, wt_vars)

vars # show selected vars
```

### Munge the data  
The code below executes the grunt work in the analysis. A couple of key details include:

- We'll keep variables for age and age/10. The latter gives us our model a more meaningful coefficient for age, since each single year of age changes the probability of retirement just a little. 
- The filter on the `year` variable narrows the data to just 2000 to 2016. Picking a relatively narrow range of time helps control for the effect of political and other broad social changes on retirement trends. 
- The code below drops the nows that are NA, "Don't know", or "Inapplicable." It's possible that these rows might carry some information and could help the model. But in this analysis I've made the choice to use only rows where the respondent intentionally provided a specific answer. 

```{r first_munge, cache = TRUE}
retire <- gss_all %>%
  select(one_of(vars)) %>%
  drop_na(wrkstat, year, age, born, wrkgovt) %>% # these are the vars that stuck
  filter(year >= 2000, year <= 2016) %>% # selected date range
  mutate(age_dec = age/10, # create a "decade of age" column
         is_retired = ifelse(wrkstat == "retired", 1, 0), # create the response var
         born = fct_recode(born, "Native born" = "yes", "Foreign born" = "no", 
                           NULL = "iap", NULL = "dk", NULL = "na"),
         born = fct_drop(born), 
         sex = fct_recode(sex, "Male" = "male", "Female" = "female"),
         sex = fct_drop(sex), 
         wrkgovt = fct_recode(wrkgovt, "Private sector" = "private", 
                              "Public sector" = "government", 
                           NULL = "iap", NULL = "dk", NULL = "na"),
         wrkgovt = fct_drop(wrkgovt), 
         sex_born = interaction(sex, born, sep = ", ")) 

# Check the number of rows remaining
nrow(retire) 
```

### Coerce to survey design  
The following code chunk creates a complex survey design that maximizes the accuracy of inference  when applied to the general U.S. population. Curious readers can get the details [on the homepage of the Thomas Lumley's `survey` package](http://r-survey.r-forge.r-project.org/survey/). 
```{r survey_design}
retire_svy <<- retire %>%
    mutate(stratvar = interaction(year, vstrat)) %>%
    as_survey_design(id = vpsu,  
                     strata = stratvar,
                     weights = wtssall,
                     nest = TRUE)

head(retire_svy)
```

## Summary statistics 
### Effects of weights  
Here's a quick look at how different our results will be when we use a complex survey design. 

#### Munge to get weighted and nonweighted data  
```{r demo_plot_data}
options(survey.lonely.psu="adjust")

raw_data <- retire %>%
  group_by(wrkstat) %>%
  summarise(n = n()) %>%
  mutate(prop = n/sum(n))

adj_data <- retire_svy %>%
    filter(wrkstat != "IAP") %>%
    group_by(wrkstat) %>%
    summarise(adj_prop = survey_mean(na.rm = TRUE, vartype = "ci"))

demo_data <- raw_data %>%
  left_join(adj_data) %>%
  gather(type_of_prop, prop, -n, -wrkstat, -adj_prop_low, -adj_prop_upp) %>%
  mutate(type_of_prop = fct_recode(type_of_prop, "Unweighted" = "prop", "Weighted" = "adj_prop")) %>%
  arrange(wrkstat)

head(demo_data, 5)
```

#### Barplot 
The effects are small but noticeable. In particular, the data has more retired people then the general population. Our complex survey design will ensure that this fact doesn't distort our inference. 

```{r fig.showtext=TRUE, fig.dim = c(6, 4)}
ggplot(demo_data, aes(x = reorder(wrkstat, -adj_prop_upp), y = prop, fill = type_of_prop)) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_fill_manual(values = wes_palette("Zissou1")[2:3]) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, .6)) +
  scale_x_discrete(labels = scales::wrap_format(10)) +
  theme(axis.text.x = element_text(size = 5)) +
  labs(title = "How GSS Weights Change the Distribution \nof Employment Status", 
       y = "Proportion of respondents", 
       x = "Employment status", 
       fill = "Sampling \nmethod")
```

### Age distributions of retired people 
Later on in this analysis, we'll see statistically signicant differences in the probability of retirement depending on the respondent's sex and whether they were born in the U.S. The plots below are intended to put those conclusions into context.

#### Boxplot by sex & birth country
Note that this plot uses the unweighted dataframe `retire`, just to allow nicer ggplot formatting. A plot using the `svyboxplot()` function looks nearly identical. 

```{r fig.showtext=TRUE, fig.dim = c(6, 5)}
boxp_data <-  retire %>%
    filter(wrkstat != "IAP", is_retired == 1) %>%
    mutate(sex_born = interaction(sex, born, sep = ", "))

ggplot(boxp_data, aes(x = sex_born, y = age, fill = sex_born)) +
    scale_fill_manual(values = wes_palette("Royal1"), guide = FALSE) +
    scale_x_discrete(labels = scales::wrap_format(5)) +
    geom_boxplot() +
    labs(title = "Age Distribution of Retired Persons \nBy Sex & Country of Birth", 
         subtitle = glue("n=", nrow(boxp_data)),
         y = "Age of Respondent", 
         x = "", 
         fill = "")
```


#### Pairwise t-test
As the plot above suggests, at least one pair of means is significantly different from one another. But the difference is small: with 95% confidence, the difference between the mean ages lies between about four months (.33 times a year) and 18 months (1.5 times a year).
```{r svyttest}
svyttest(age~sex_born, retire_svy)
```

#### Density plot of age by sex  
The density plot below sheds some light on how it comes to be that men are more likely to be retired at every age. The density plot of age for retired men looks like the plot for retired women, but pushed a few years younger. Some articles has suggested this happens because [women's savings lag behind men's](https://money.cnn.com/2018/07/12/retirement/women-men-retirement/index.html). 
```{r density_plot, fig.showtext=TRUE}
# filter to show only retired people
dens_plot_data <- retire %>%
  filter(is_retired == 1) 

# plot distributions
ggplot(dens_plot_data, aes(x = age, fill = sex)) +
    geom_density(alpha = .5) +
    scale_color_manual(aes(alpha = .6)) +
    scale_fill_manual(values = wes_palette("Zissou1")[2:3]) +
    labs(title = "Age Distribution of Retired Respondents \nby Gender", 
         subtitle = glue("n =", scales::comma(nrow(dens_plot_data))), 
         fill = "Gender", 
         y = "Density", 
         x = "Age") +
    theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

## Part III: Simple model
We know there's a relationship between age and the probability of retirement: We expect the probability to increase with age. But a model that includes _only_ age might miss subtler relationships like the one with sex that we just plotted. How well can we predict the probability of retirement using age alone? Pretty well, it turns out. 

### Age-only model
#### Fit logistic model
```{r fit_simple}
#options(survey.lonely.psu="adjust")

simple_model <- svyglm(is_retired~age_dec, 
                  family = "quasibinomial", 
                  design = retire_svy)
```

#### View coefficients
```{r view_simple}
# The jtools library gives us a few key functions for summarizing logistic models 
# created with the svyglm() function. 
library(jtools) 
summ(simple_model)
```

#### Write out and interpret
For every 10-year increase in age, the odds of being retired increase by a factor of about 6 (e^1.76).

$$
\text{odds}: (\frac {\pi}{1 - \pi}) = e^{-12.01 + 1.76(age/10)}
$$
#### Examine predictive power
The code below uses the model to predict whether each person in the data is retired. For now, we'll set the cutoff at .5 — so if the model says your probability of being retired is 0.51, we'll guess that you are retired. 
```{r predict_simple}
retire$fitted <- fitted(simple_model)
retire$predicted <- as.numeric(retire$fitted >= 0.5)
head(retire$predicted, 25)
```

#### Sensitivity and specificity
When we set the cutoff at 0.5, we guess the retirement status of the non-retired majority of cases correctly 96.0% of the time. But for the minority of respondents who are retired, we're guessing right only 61.1% of the time. 
```{r}
cutoff <- 0.5
truth <-factor(retire$is_retired, labels = c("not retired", "retired"))
prediction <- factor(fitted(simple_model)>cutoff, labels =c("not retired", "retired"))

sensitivity(data = prediction, reference = truth, positive = "retired")
specificity(data = prediction, reference = truth, negative = "not retired")
```

#### Compute confidence bands 
Our n is large enough to support 99% confidence here, rather than the more standard 95% confidence. 

```{r}
# set up predictor space
smooth_ages <- data.frame(age_dec = pretty(1.8:10.0, 100)) 
# compute predictions
smooth_predict <- predict(simple_model, 
                        newdata = smooth_ages,
                        type = "response", se = TRUE)
# format as a dataframe
simp_plot_data <- data.frame(smooth_predict, age_dec = smooth_ages)
# add data for confidence bands
simp_plot_data <- simp_plot_data %>%
  mutate(LoCI = response - qnorm(0.995) * SE, 
         HiCI = response + qnorm(0.995) * SE)
```

#### Plot age with confint

```{r fig.showtext=TRUE, fig.height=4, fig.width = 6}
ggplot(simp_plot_data, aes(x = age_dec, y = response)) + 
    scale_x_continuous(labels = scales::comma_format(suffix = 0), limits = c(4.0, 10.0)) +
    scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
    geom_ribbon(aes(ymin = LoCI, ymax = HiCI), stat = "identity", alpha = .2) +
    geom_line(colour = wes_palette("Royal1")[2],  size = 1) +
    labs(title = "Probability of Retirement as a Function of Age", 
         subtitle = glue("99% confidence; ", "n = ", scales::comma(nrow(retire))), 
         y = "Probability of retirement", 
         x = "Age", 
         caption = "Source: General Social Survey (date range: 2000 to 2016)")
```

## Part IV: More complex models

### Model with age & sex  
We already showed above that sex helps explain whether we can expect a given person to be retired or not. How does the model change if we add in sex and an interaction between sex and age?

#### Fit logistic model
```{r fit_age_sex_model}
model1 <- svyglm(is_retired~age_dec+sex+age_dec*sex,
                family = "quasibinomial",
                design=retire_svy)
```

#### Conduct F-test on interaction term  
The results are not ambiguous. Age, sex, and the interaction between them are all highly statistically significant. The presence of an interaction term here means that the effect of age on the response is different for men and women.

```{r}
anova(model1, test = "F")
```

#### View coefficients  
One thing that's tricky here is that, if we didn't include the interaction, the coefficient on `sexFemale` would be negative. That means we expect the probability of retirement for a woman to be lower than for a man of the same age. This expectation is still true in the model with the interaction because the interaction term has a negative coefficient. 

```{r}
summ(model1)
```

#### Write out and interpret
$$
\log(\frac {\pi_{retired}}{1 - \pi_{retired}}) = \alpha + \beta_{1 \ age} + \beta_{2 \ sex} + \beta_{3 \ sex \ * \ age}
$$

#### Examine predictive power
None of these predictions are changed by the more complex model. 
```{r fit_model1}
retire$fitted1 <- fitted(model1)
retire$predicted1 <- as.numeric(retire$fitted1 >= 0.5)
head(retire$predicted1, 25)
```

#### Sensitivity and specificity
When we set the cutoff at 0.5, we guess the retirement status of the non-retired majority of cases correctly 95.8% of the time — this is actually slightly _worse_ than what we got using the simple model. But for the minority of cases that are retired, we're guessing right only 63.2% of the time. That's a significant improvement over the simple model. 

I know this is screaming for an ROC curve, and we'll see one a little later in the analysis, when all three models are available. 
```{r sens_model1}
cutoff <- 0.5
truth <-factor(retire$is_retired, labels = c("not retired", "retired"))
prediction <- factor(fitted(model1)>cutoff, labels =c("not retired", "retired"))

sensitivity(data = prediction, reference = truth, positive = "retired")
specificity(data = prediction, reference = truth, negative = "not retired")
```

#### Compute confidence bands
```{r fig.height= 4, fig.width=6}
comp_predictors <- expand.grid(
  age_dec = seq(1.8, 9.0, length.out = 100), 
  sex = c("Male", "Female"))

comp_predict <- predict(model1, newdata = comp_predictors, type = "response")

comp_plot_data <- data.frame(comp_predict, 
                             age_dec = comp_predictors$age_dec, 
                             sex = comp_predictors$sex)

comp_plot_data <- comp_plot_data %>%
  mutate(LoCI = response - qnorm(0.995) * SE, 
         HiCI = response + qnorm(0.995) * SE)
```

#### Plot 
```{r fig.showtext=TRUE, fig.height= 4, fig.width=6}
ggplot(comp_plot_data, aes(x = age_dec, y = response, group = sex)) + 
    scale_color_manual(values = wes_palette("Royal1")[1:2]) +
    scale_x_continuous(labels = scales::comma_format(suffix = 0), limits = c(4.0, 9.0)) +
    scale_y_continuous(labels = scales::percent, limits = c(0, 1)) +
    geom_vline(xintercept = 7.0, linetype = 2) +
    geom_ribbon(aes(ymin = LoCI, ymax = HiCI), stat = "identity", alpha = .20) +
    geom_line(aes(color = sex), size = 1) +
    labs(title = "Relationship Between Age, Sex, \nand Probability of Being Retired", 
         subtitle = glue("n = ", scales::comma(nrow(retire)), "; Date range = 2000 to 2016"), 
         y = "Probability of being retired", 
         x = "Age of Respondent", 
         color = "Gender", 
         caption = "Source: General Social Survey (date range: 2000 to 2016)") +
  theme(legend.position="right")
```

### Model with age, sex, and country of birth  
It stands to reason that country of birth might affect the probability of retirement. Immigrants often have fewer resources and savings, and might end up working longer. Let's use logistic regression to investigate this idea. 

#### Fit logistic model
```{r}
options(survey.lonely.psu="adjust")

model2 <- svyglm(is_retired~age_dec+sex+born+wrkgovt
                +age_dec*sex
                +wrkgovt*sex,
                family = "quasibinomial", 
                design=retire_svy)
```

#### Conduct F-test on interactions
All of the coefficients on parameters and interactions are statistically significant. 
```{r}
anova(model2, test = "F")
```

#### View coefficients
```{r}
summ(model2)
```

#### Show formulas
Here's how our model looks as a formula. 
$$
\log(\frac {\pi_{retired}}{1 - \pi_{retired}}) = \alpha + \beta_{1 \ age} + \beta_{2 \ sex} + \beta_{3 \ birthplace} + \beta_{4 \ govt} +\beta_{5 \ sex \ * \ age}+\beta_{6 \ sex \ * \ govt}
$$

#### Plot marginal effects  
Thanks to [Kieran Healy](https://kieranhealy.org/)'s book _Data Visualization_ for introducing me to this way of plotting coefficients. 
```{r fig.showtext=TRUE, fig.height=3.5, fig.width=5.25}
# convert model to a dataframe
t_model <- tidy(model2, conf.int = TRUE) 
# remove the alpha term from the data
t_model <- t_model %>%
  filter(term != "(Intercept)")
# write human-readable labels
t_model$labels <- c("Age/10", "Sex: Female", "Birthplace: Foreign", 
                    "Private employee",  "Age/10*Female", "Private*Female")
# plot
ggplot(t_model, mapping = aes(x = labels, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() +
  scale_y_continuous(limits = c(-4, 4)) +
  labs(title = "Marginal Effects of Model Terms", 
       y = "Value of Coefficient",
       x = "") +
  coord_flip()
```

#### Examine predictive power
Most predictions will not be changed by making the model more complex.  
```{r predict_model2}
retire$fitted2 <- fitted(model2)
retire$predicted2 <- as.numeric(retire$fitted2 >= 0.5)
head(retire$predicted2, 25)
```

#### Sensitivity and specificity
The additional parameters to the model further decrease the specificity and increase the sensitivity. 

```{r sens_model2}
cutoff <- 0.5
truth <-factor(retire$is_retired, labels = c("not retired", "retired"))
prediction <- factor(fitted(model2)>cutoff, labels =c("not retired", "retired"))

sensitivity(data = prediction, reference = truth, positive = "retired")
specificity(data = prediction, reference = truth, negative = "not retired")
```

#### ROC curves
```{r roc_prep}
df <- data.frame(response = retire$is_retired, 
                 simple_model =fitted(simple_model), 
                 model1 =fitted(model1), 
                 model2 = fitted(model2))

df_long <- df %>%
  gather(model1, model2, simple_model, key = "model", value = "predictions")

head(df_long, 3)
```

I'm plotting a zoomed-in ROC curve here. If we don't zoom in, then all three curves look exactly the same, which goes to show that the simple model with age alone is nearly as good as the more complex ones for making predictions (on training data, in this case). 

```{r plot_roc, fig.showtext=TRUE}
library(plotROC)

p <- ggplot(df_long, aes(d = response, m = predictions, color = model)) + 
        geom_roc(cutoffs.at = c(0.75, 0.65, 0.5, 0.25), labelround = 2) +
        labs(title = "ROC Curve for Model with Weight and Color")

p1 <- ggplot(df_long, aes(d = response, m = predictions, color = model)) + 
        scale_x_continuous(limits = c(0, .25)) +
        scale_y_continuous(limits = c(.75, 1)) +
        geom_roc(cutoffs.at = c(0.33, 0.25, 0.20), labelround = 2) +
        labs(title = "ROC Curve for Model with Weight and Color")

p1
```

An area-under-the-curve calculation shows that the model with age and sex is the best out of the three for making predictions.  
```{r auc}
calc_auc(p)
```

#### AIC
While adding complexity to the model doesn't necessarily improve our classification accuracy with this data, it does reduce our Akaike information criterion score. With this measure, it's the more complicated model that shows up as the best.  
```{r aic}
AIC(simple_model)
AIC(model1)
AIC(model2)
```
#### Predict for age, born & self-employment
```{r prep_faceted_plot}
comp_predictors <- expand.grid(
  age_dec = seq(1.8, 9.0, length.out = 100), 
  sex = c("Male", "Female"), 
  born = c("Native born", "Foreign born"), 
  wrkgovt = c("Public sector", "Private sector"))

comp_predict <- predict(model2, newdata = comp_predictors, type = "response")

comp_plot_data <- data.frame(comp_predict, 
                             age_dec = comp_predictors$age, 
                             sex = comp_predictors$sex, 
                             born = comp_predictors$born, 
                             wrkgovt = comp_predictors$wrkgovt)

comp_plot_data <- comp_plot_data %>%
  mutate(LoCI = response - qnorm(0.995) * SE, 
         HiCI = response + qnorm(0.995) * SE)
```

#### Plot  
There are big, statistically significant differences not only between men and women, but also between immigrants and native-born U.S. citizens and between public-sector and private-sector employees. 

```{r fig.showtext=TRUE, fig.height= 5, fig.width=6.5}
ggplot(comp_plot_data, aes(x = age_dec, y = response, group = sex)) + 
    scale_color_manual(values = wes_palette("Royal1")[1:2]) +
    scale_x_continuous(labels = scales::comma_format(suffix = 0), limits = c(4.0, 9.0)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1), limits = c(0, 1),  
                       breaks = seq(.1, 1, .2)) +
    geom_ribbon(aes(ymin = LoCI, ymax = HiCI), stat = "identity", alpha = .10) +
    geom_line(aes(color = sex), size = 1) +
    facet_grid(born ~ wrkgovt) +
    geom_vline(xintercept = 7.0, linetype = 3) +
    labs(title = "Relationship Between Probability of Being Retired \nand Age, Sex, Birthplace & Employer", 
         subtitle = "99% confindence",
         y = "Probability of being retired", 
         x = "Age of Respondent", 
         color = "Gender", 
         caption = "Source: General Social Survey (date range: 2000 to 2016)") +
  theme(legend.position="right")
```

